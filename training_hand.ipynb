{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataset.ganhands import GanHands\n",
    "from dpt.models import HandModel\n",
    "from dpt.transforms import Resize\n",
    "import torch.utils.data as data\n",
    "from torchvision.transforms import Compose\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from dpt.transforms import PrepareForNet\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "net_w = 384\n",
    "net_h = 384\n",
    "transforms = Compose([\n",
    "    Resize(\n",
    "        net_w,\n",
    "        net_h,\n",
    "        resize_target=None,\n",
    "        keep_aspect_ratio=True,\n",
    "        ensure_multiple_of=32,\n",
    "        resize_method=\"upper_bound\",\n",
    "        image_interpolation_method=cv2.INTER_CUBIC,\n",
    "    ),\n",
    "    PrepareForNet()\n",
    "])\n",
    "\n",
    "training_dataset = GanHands('/home/zain/University/Datasets/GANeratedHands_Release/data/noObject/0001', 1024, transforms)\n",
    "validation_dataset = GanHands('/home/zain/University/Datasets/GANeratedHands_Release/data/noObject/0002', 1024, transforms)\n",
    "\n",
    "training_loader = data.DataLoader(\n",
    "    training_dataset, batch_size=1, num_workers=0, shuffle=False, pin_memory=True\n",
    ")\n",
    "validation_loader = data.DataLoader(\n",
    "    validation_dataset, batch_size=1, num_workers=0, shuffle=False, pin_memory=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "HandModel(\n  (encoder): VisionTransformer(\n    (patch_embed): PatchEmbed(\n      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    )\n    (pos_drop): Dropout(p=0.0, inplace=False)\n    (blocks): ModuleList(\n      (0): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (1): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (2): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (3): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (4): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (5): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (6): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (7): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (8): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (9): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (10): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n      (11): Block(\n        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (attn): Attention(\n          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n          (attn_drop): Dropout(p=0.0, inplace=False)\n          (proj): Linear(in_features=768, out_features=768, bias=True)\n          (proj_drop): Dropout(p=0.0, inplace=False)\n        )\n        (drop_path): Identity()\n        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n        (mlp): Mlp(\n          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n          (act): GELU()\n          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n          (drop): Dropout(p=0.0, inplace=False)\n        )\n      )\n    )\n    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n    (pre_logits): Identity()\n    (head): Linear(in_features=768, out_features=1000, bias=True)\n  )\n  (head): Sequential(\n    (0): Linear(in_features=1000, out_features=512, bias=True)\n    (1): ReLU()\n    (2): Dropout(p=0.2, inplace=False)\n    (3): Linear(in_features=512, out_features=512, bias=True)\n    (4): ReLU()\n    (5): Dropout(p=0.2, inplace=False)\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=512, out_features=512, bias=True)\n    (9): ReLU()\n    (10): Dropout(p=0.2, inplace=False)\n    (11): Linear(in_features=512, out_features=512, bias=True)\n    (12): ReLU()\n    (13): Dropout(p=0.2, inplace=False)\n    (14): Linear(in_features=512, out_features=63, bias=True)\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_model = HandModel()\n",
    "hand_model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss function"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Adam\n",
    "optimizer = torch.optim.Adam(\n",
    "    [\n",
    "        {'params': hand_model.encoder.parameters()},\n",
    "        {'params': hand_model.head.parameters(), 'lr': 1e-4}\n",
    "    ], lr=1e-6\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Loop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## One Epoch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def train_one_epoch(model, training_loader, optimizer, device):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        inputs = data[\"image\"]\n",
    "        y_true = data[\"coords\"]\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, y_true)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 8 == 7:\n",
    "            last_loss = running_loss / 20 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "\n",
    "    return last_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training over epochs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.95 GiB total capacity; 2.95 GiB already allocated; 29.81 MiB free; 3.04 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [14]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001B[39;00m\n\u001B[1;32m     11\u001B[0m hand_model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m---> 12\u001B[0m avg_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhand_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;66;03m# We don't need gradients on to do reporting\u001B[39;00m\n\u001B[1;32m     15\u001B[0m hand_model\u001B[38;5;241m.\u001B[39mtrain(\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36mtrain_one_epoch\u001B[0;34m(model, training_loader, optimizer, device)\u001B[0m\n\u001B[1;32m     15\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Make predictions for this batch\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Compute the loss and its gradients\u001B[39;00m\n\u001B[1;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(outputs, y_true)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m~/PycharmProjects/DPT/dpt/models.py:195\u001B[0m, in \u001B[0;36mHandModel.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    194\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 195\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead(x)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/timm/models/vision_transformer.py:374\u001B[0m, in \u001B[0;36mVisionTransformer.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 374\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    375\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhead(x)\n\u001B[1;32m    376\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/timm/models/vision_transformer.py:367\u001B[0m, in \u001B[0;36mVisionTransformer.forward_features\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    364\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpos_drop(x)\n\u001B[1;32m    366\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks:\n\u001B[0;32m--> 367\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    369\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x)[:, \u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    370\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_logits(x)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/timm/models/vision_transformer.py:199\u001B[0m, in \u001B[0;36mBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m    198\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_path(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x)))\n\u001B[0;32m--> 199\u001B[0m     x \u001B[38;5;241m=\u001B[39m x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop_path(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnorm2\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/timm/models/vision_transformer.py:147\u001B[0m, in \u001B[0;36mMlp.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 147\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(x)\n\u001B[1;32m    149\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop(x)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:889\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    888\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 889\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    890\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m itertools\u001B[38;5;241m.\u001B[39mchain(\n\u001B[1;32m    891\u001B[0m         _global_forward_hooks\u001B[38;5;241m.\u001B[39mvalues(),\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks\u001B[38;5;241m.\u001B[39mvalues()):\n\u001B[1;32m    893\u001B[0m     hook_result \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m, result)\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/linear.py:94\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m---> 94\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.local/lib/python3.9/site-packages/torch/nn/functional.py:1753\u001B[0m, in \u001B[0;36mlinear\u001B[0;34m(input, weight, bias)\u001B[0m\n\u001B[1;32m   1751\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight):\n\u001B[1;32m   1752\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(linear, (\u001B[38;5;28minput\u001B[39m, weight), \u001B[38;5;28minput\u001B[39m, weight, bias\u001B[38;5;241m=\u001B[39mbias)\n\u001B[0;32m-> 1753\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 3.95 GiB total capacity; 2.95 GiB already allocated; 29.81 MiB free; 3.04 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "epoch_number = 0\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    hand_model.train(True)\n",
    "    avg_loss = train_one_epoch(hand_model, training_loader, optimizer, device)\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    hand_model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    i = 0\n",
    "    for vdata in validation_loader:\n",
    "        vinputs, vlabels = vdata\n",
    "        voutputs = hand_model(vinputs)\n",
    "        vloss = loss_fn(voutputs, vlabels)\n",
    "        running_vloss += vloss\n",
    "        i += 1\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(hand_model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}